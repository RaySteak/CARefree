Camera:
- By tinkering with the camera we can notice that it can only output 2 resolutions (QVGA and VGA),
and 2 color formats (RGB565 and grayscale). By default, VGA at RGB565 is out of question as
the buffer needed would be larger than the RAM memory the board has.
- How do we choose from the remaining 3 possibilities then? Well, initially, it would be ideal to
use the VGA grayscale setting (second highest memory usage after RGB565 VGA) since colours
don't really help us as the camera has pretty bad colour recognition (as tested). Still, this
would leave us with very little remaining memory, since the FreeRTOS components already eat up
some memory. We don't need that high of resolution anyway, as QVGA (320 x 240 is) more than enough
and even most big classification models use a smaller input size than this. As already mentioned,
grayscale would be preferable, but the built-in MSR model takes as input 3-channel RGB images, so
we will choose RGB565 QVGA and grayscale it before sending to the edge (combined with jpg
conversion and/or further downscaling)
- It seems that with QVGA RGB565, applying 4-size average pooling causes very bad noise artifacts,
and just picking the top-left value is preferable and yields better results (fixed, look 2 bullet
points ahead and in the avg_pool_rgb565 for explanation of bug).
- Well, as it turns out, the camera can also do QQVGA (160 * 240). Since we were already doing
downscaling to QQQVGA (with 4-size pooling kernel), and having the impossibility to send QQVGA
images to the server when taking QVGA images as it would require too much RAM storing both a QVGA
and QQVGA image, we will pick QQVGA as the resolution of choice as it is favourable from every
point of view for the grayscale case. Problem is, for the RGB565 case for the the built-in face
detection model, it doesn't like to receive QVGA as input as it is too small for it. Upscaling it
to fit the minimum size would use up too much memory as we would be roughly in the QQVGA + QVGA
images at once AND in the RGB565 format.
- There was a bug in the average pool function for RGB565 images related to the fact that RGB565
is actually big endian. While fixing this bug, I also found that if I want to use face detection
anymore I have to disable MQTT (by commenting the initialization function) as it seemed to use up
quite some RAM and inference with the face detection model would crash. So, there is no possibility
of testing this method with the whole pipeline anymore. Still, the implementation should work like
a breeze for boards with enough memory.

Labeling data:
Using a pre-trained model from huggingface running on the edge as ground truth is the way to go,
otherwise there is no way to label the images.

Feature extraction and classification:
- Ideally, we would need a very small (maybe 70KB or less) pre-trained CNN model to do transfer learning
on it as any end-to-end training on the ESP32 is out of the question (both memory-wise and implementation-wise,
as backpropagation and gradient descent would need to be implementeed). Even with quantization and pruning
we weren't able to find one that small. Smallest we could find is the PrunaAI SqueezeNet.
What about implementing and training our own model? Too much of a hassle too as it would require additional
tinkering with tensorflow lite micro and the model wouldn't learn much anyway.
- Solution: extract some features from the image and then train a very simple classifier. Best idea would
be to use some very toned-down HoG-style features
- For classifier, it would be ideal to use a one-layer NN, but that might be too big.
- 2-layer NN with HoG seems to work fine, ignore previous statement as it was made while the project was still
in its infancy.

Further optimizations:
- Tried using async http requests. The ESP library only supports them if HTTPS is used. To avoid using a proper
certificate these variables have to be set in sdkconfig: 'CONFIG_ESP_TLS_INSECURE=y' and
'CONFIG_ESP_TLS_SKIP_SERVER_CERT_VERIFY=y'. Still, the request doesn't work and returns only 'EAGAIN' immediately
every time upon performing the request. Spent a lot of time trying to debug to see what is wrong but to no avail.
At first thought it was a bug having to do with the combination of async and disabled verification, but the issue
is even weirder because it still doesn't work even with verification enabled. Upon further debugging, it seems
the issue comes from line 1448 in esp_http_client.c where the function esp_transport_connect_async is called.
This in turn calls an underlying internal function '_connect_async' which seems to be builtin and inaccessible.
So the issue is probably a bug of the implementation for the heltec lora v3 board.

General consensus: An ESP32-CAM featuring the PSRAM (also known as SPIRAM) module would have been able to achieve
much more, and way fewer compromises would have been made

MQTT:
- QOS 2 is the best option. QOS 1 would be the worst because we don't want to send
duplicate weights for sure. QOS 0 would also work but since the board worked so hard to update the weights anyway,
let's make sure they get to the server. Problem is, with QOS 1 and 2, the outbound messages are stored in an
outbox until sent which uses up too much memory, so we will make do with QOS 0.
- To make sure model doesn't get updated during gradient descent and vice-versa, synchronization is used. This is
done optimally (from the prerspective of both code size and efficiency) and explained in the code through comments
(see MQTT event handler and train task sections). Currently, for choice of simplicity, images received during model
update are just discarded (IDEA: while model is being updated, stop taking photos and basically halt all code).

Accelerometer:
- Typical car acceleration during normal drive is 0.14g. ADXL345 interrupt threshold is 62.5mg/LSB = 0.0625g/LSB.
So, in theory the smallest setting should be able to detect small accelerations that happen fairly frequently
during driving. Any higher and it won't trigger enough (especially if the rented car is slow).
- This accelerometer posed a lot of problems. First, the library I was able to find is very lacking and needed
some extra additions to use the interrupts. Nothing a little datasheet searching won't fix. Furthermore,
the i2c driver the library comes with is also very bad, and it seems it missed some things like pad_select which
rendered it useless on some pins until the change was added. Another problem seems to be that it loses connection
almost all the time. The measurements seem ok (albeit a bit off: 1.07g for Z, 0.03g for X and Y) when the board
is still, but as soon as there is some moevement, the read errors appear, and it continues until the ESP is reset.
Sometimes, there is not read error but the value is just off by a lot, like 40g for Z.
- Problems aside, the settings chosen for the accelerometer are: full resolution at 13 bit, 3.9mg/LSB, as using
higher resolution does not affect power draw. For the power mode, 50Hz is chosen for now, though a sampling rate
that high is not needed anyway, and low-power mode is also set to false as the gains are not that great and it
also limits output frequency range and adds noise.
- Ok, so the software driver was the issue apparently, no more desync problem when switching to i2c hardware driver
with the builtin "i2c.h" library. First problem encountered though is that after a lot of struggle of using the newer
"i2c_master.h" library, it ended up not working and throwing errors that the old library was in use. Doing some
digging into the code of "esp_camera.h" it is easy to find that it uses the old "i2c.h" driver. Sadly, these
2 drivers can't coexist on the same application, so the only solution was to use the old driver library for
the accelerometer as well.
- There are still problems with the accelerometer, as the interrupt doesn't seem to work at all, and it always
outputs that activity is detected regardless of the threshold or participating axes set. Changing the interrupt
pin works, as only the active port seems to be always on, and the inactive one is off as expected.
- It seems the interrupt of the accelerometer needs to be cleared. Thing is, there are 2 ways to clear it, either
reading the 6 byte data register starting at 0x32 or reading the interrupt source register at 0x30. The former
did not work at all which first prompted me to think the first accelerometer was broken. The latter works, but
the threshold had to be set higher (6 * 62.5mg) as just the idle noise was triggering it.
- Future improvements would be to add a way to better synchronize rounds between devices. At first it seemed like
a good idea to make the ESP wait for a model after it wakes up (apart from when it's on first round). This idea
was quickly scrapped because then very few device would be sending to the cloud then and would just be waiting for
a model. It should be up to the cloud to decide how many devices need to have sent the model so that it runs
aggregation. Another improvement is to look into sleep modes and go into either light sleep or (prefferably) deep
sleep using one of the 2 EXT pins from the suite of RTC pins.